{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiaxuan/.pyenv/versions/3.6.5/lib/python3.6/site-packages/spacy/util.py:1176: ResourceWarning: unclosed file <_io.BufferedReader name='/Users/jiaxuan/.pyenv/versions/3.6.5/lib/python3.6/site-packages/en_core_web_sm/en_core_web_sm-3.0.0/tok2vec/model'>\n",
      "  reader(path / key)\n",
      "/Users/jiaxuan/.pyenv/versions/3.6.5/lib/python3.6/site-packages/spacy/util.py:1176: ResourceWarning: unclosed file <_io.BufferedReader name='/Users/jiaxuan/.pyenv/versions/3.6.5/lib/python3.6/site-packages/en_core_web_sm/en_core_web_sm-3.0.0/tagger/model'>\n",
      "  reader(path / key)\n",
      "/Users/jiaxuan/.pyenv/versions/3.6.5/lib/python3.6/site-packages/spacy/util.py:1176: ResourceWarning: unclosed file <_io.BufferedReader name='/Users/jiaxuan/.pyenv/versions/3.6.5/lib/python3.6/site-packages/en_core_web_sm/en_core_web_sm-3.0.0/senter/model'>\n",
      "  reader(path / key)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from textblob import TextBlob\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import collections\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_trf\", disable=['tok2vec', 'lemmatizer', 'ner']) # accuracy\n",
    "#nlp = spacy.load('en_core_web_sm', disable=['tok2vec', 'lemmatizer', 'ner']) # efficiency\n",
    "nlp = spacy.load('en_core_web_sm', disable = ['lemmatizer'])\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "def language_detection(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date range\n",
    "start_date = \"2019-01-01\"\n",
    "end_date = \"2021-12-31\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define keywords: Hong Kong\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## aspect\n",
    "transport = ['location', 'bus', 'buses', 'busstop', 'mrt', 'station', 'stations', 'walk', \n",
    "            'attraction', 'attractions', \n",
    "            'center', 'centers', 'centre', 'centres', 'connections', 'connection', 'connectivity', 'court', \n",
    "            'direction', 'directions', 'ferry', \n",
    "            'inconvenience', 'inconvenient', 'convenience', 'convenient',\n",
    "            'min', 'mins', 'minute', 'minutes', 'parking', 'proximity', 'ride', \n",
    "            'stop', 'stops', 'train', 'trains', \n",
    "            'transport', 'transportation', 'transports', 'mrts', 'tube', 'tubes', 'commuting', 'walking'\n",
    "            'accessible', 'convenient', 'inconvenient', 'close', 'far', 'walkable', 'near', 'central',\n",
    "            'connected', 'underground', 'undergrounds', 'subway', 'subways', 'subwaystop'\n",
    "            ]\n",
    "\n",
    "amenities = ['amenities', 'cafe', 'coffee', 'club', 'clubs', 'eateries', 'food', 'foods', 'foodcourt', \n",
    "            'fruit', 'grocery', 'groceries', \n",
    "            'hawker', 'hawkers', 'mall', 'malls', 'market', 'markets', 'restaurant', 'restaurants', \n",
    "            'shopping', 'shops', 'store', 'stores',\n",
    "            'supermarket','eats', 'courts', 'foodcourts', 'bar', 'bars', 'mart', 'marts',\n",
    "            'nightlife', 'plaza', 'plazas', 'coffeeshop', 'coffeeshops',\n",
    "            'commercial', 'reachable', 'eatery',\n",
    "            'theater', 'theatre', 'cinema', 'coffeehouse', 'coffeehouses', 'coffeeplaces'\n",
    "            ]\n",
    "\n",
    "greenery = ['garden', 'gardens', 'greeneries', 'greenery', 'park', 'green', 'tree']\n",
    "\n",
    "safety = ['safety', 'security', 'dark', 'safe', 'unsafe', 'dangerous']\n",
    "\n",
    "noise = ['noise', 'noises', 'noisy']\n",
    "\n",
    "\n",
    "## description\n",
    "transport_des = ['accessible', 'convenient', 'inconvenient', 'close', 'far', 'walkable', 'near', \n",
    "                'central','connected', 'walking']\n",
    "\n",
    "amenities_des = ['commercial', 'reachable']\n",
    "\n",
    "greenery_des = ['green']\n",
    "\n",
    "safety_des = ['dark', 'safe', 'unsafe', 'dangerous']\n",
    "\n",
    "noise_des = ['noisy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The filtered dataset has 414923 rows and 6 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiaxuan/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/Users/jiaxuan/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# import airbnb dataset\n",
    "\n",
    "df = pd.read_csv('/Users/jiaxuan/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/MUP_year2/dissertation/datasets/nyc/reviews.csv')\n",
    "\n",
    "# filter 2019-2021 (3 years) [df_fl]\n",
    "after_start_date = df[\"date\"] >= start_date\n",
    "before_end_date = df[\"date\"] <= end_date\n",
    "between_two_dates = after_start_date & before_end_date\n",
    "\n",
    "df_fl = df.loc[between_two_dates]\n",
    "\n",
    "# checking shape\n",
    "print(\"The filtered dataset has {} rows and {} columns.\".format(*df_fl.shape))\n",
    "\n",
    "# drop na\n",
    "df_fl.dropna(inplace=True)\n",
    "\n",
    "# create column language\n",
    "df_fl['language'] = df_fl['comments'].apply(language_detection)\n",
    "df_fl_en = df_fl[(df_fl['language']=='en')]\n",
    "\n",
    "# remove '<br/>'\n",
    "df_fl_en['comments'] = df_fl_en['comments'].str.replace('<br/>', ' ', regex = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiaxuan/.pyenv/versions/3.6.5/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# sentencize for aspect_des\n",
    "df_fl_en['comments'] = df_fl_en[\"comments\"].apply(lambda x: [sent.text for sent in nlp(x).sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign each sentence a sentence id\n",
    "df_fl_en = df_fl_en.explode(\"comments\", ignore_index=True)\n",
    "df_fl_en.index.name = \"Sentence ID\"\n",
    "df_fl_en = df_fl_en.reset_index()\n",
    "\n",
    "# further remove non-en sentences (some comments contains multiple languages)\n",
    "df_fl_en['language'] = df_fl_en['comments'].apply(language_detection)\n",
    "df_fl_en = df_fl_en[(df_fl_en['language']=='en')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aspect_description(text):\n",
    "    doc = nlp(text)\n",
    "    aspect_des = []\n",
    "    ####description = []\n",
    "\n",
    "    for token in doc:\n",
    "        # root = adj\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            if token.head.pos_ == \"NOUN\":\n",
    "                aspect_des.append((str(token.head.text).lower(), str(token.text).lower()))\n",
    "            for child in token.children:\n",
    "                if token.head.pos_ == \"NOUN\" and child.pos_ == \"ADJ\" and child.dep_ == \"conj\":\n",
    "                    aspect_des.append((str(token.head.text).lower(), str(child.text).lower()))\n",
    "                elif token.head.pos_ == \"NOUN\" and child.pos_ == \"ADJ\" and child.dep_ == \"amod\":\n",
    "                    #aspect_des.remove((str(token.head.text).lower(), str(token.text).lower()))\n",
    "                    aspect_des.append((str(token.head.text).lower(), str(child.text).lower() + \" \" + str(token.text).lower()))    \n",
    "        \n",
    "        # root = noun\n",
    "        elif token.pos_ in (\"NOUN\", \"PRON\") and token.head.pos_ == \"AUX\":\n",
    "            for child in token.head.children:\n",
    "                if child.pos_ == \"ADJ\" and token.head.pos_ in (\"NOUN\", \"PRON\"):\n",
    "                    aspect_des.append((str(token.text).lower(), str(child.text).lower()))\n",
    "                    for gchild in child.children:\n",
    "                        if gchild.pos_ == \"ADJ\" and gchild.dep_ == \"conj\":\n",
    "                            aspect_des.append((str(token.text).lower(), str(gchild.text).lower()))\n",
    "                    \n",
    "        # root = verb\n",
    "        elif token.pos_ == \"VERB\":\n",
    "            for child in token.children:\n",
    "                if child.pos_ == \"NOUN\" and child.dep_ == \"dobj\" and token.head.pos_ == \"VERB\": \n",
    "                    aspect_des.append((str(child.text).lower(), str(token.head.text).lower()))\n",
    "                elif token.dep_ == \"conj\" and child.pos_ == \"ADJ\" and child.dep_ == \"acomp\":\n",
    "                    for gchild in token.children:\n",
    "                        if gchild.pos_ in (\"NOUN\", \"PRON\") and gchild.dep_ == \"nsubj\":\n",
    "                            aspect_des.append((str(gchild.text).lower(), str(child.text).lower())) \n",
    "        \n",
    "        else: \n",
    "            continue        \n",
    "    return aspect_des\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply func(polarity) to a sentence => overall_sentiment, to a descriptive word => polarity\n",
    "def polarity(sentence):\n",
    "    score = TextBlob(sentence).sentiment.polarity\n",
    "    return score\n",
    "\n",
    "# apply func(polarity) to a sentence => overall_sentiment, to a descriptive word => polarity\n",
    "def subjectivity(sentence):\n",
    "    score = TextBlob(sentence).sentiment.subjectivity\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fl_en['aspect_pair'] = df_fl_en['comments'].apply(aspect_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fl_en_notnull = df_fl_en[df_fl_en['aspect_pair'].map(lambda d: len(d)) > 0]\n",
    "#df_fl_en_notnull\n",
    "#df_fl_en_notnull.to_csv(\"/Users/jiaxuan/Desktop/2-2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fl_en_notnull = df_fl_en_notnull.explode(\"aspect_pair\", ignore_index=True)\n",
    "df_fl_en_notnull.index.name = \"aspect ID\"\n",
    "df_fl_en_notnull = df_fl_en_notnull.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdf = df_fl_en_notnull[\"aspect_pair\"].apply(pd.Series)\n",
    "df_fl_en_notnull[\"aspect\"] = tempdf[0]\n",
    "df_fl_en_notnull[\"description\"] = tempdf[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fl_en_notnull['overall_sentiment'] = df_fl_en_notnull['comments'].apply(polarity)\n",
    "df_fl_en_notnull['overall_subjectivity'] = df_fl_en_notnull['comments'].apply(subjectivity)\n",
    "df_fl_en_notnull['des_polarity'] = df_fl_en_notnull['description'].apply(polarity)\n",
    "df_fl_en_notnull['des_subjectivity'] = df_fl_en_notnull['description'].apply(subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fl_en_notnull.to_csv(\"/Users/jiaxuan/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/MUP_year2/dissertation/datasets/nyc/qgis/final_aspect_results/aspect_sent_output.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_fl_en_notnull['aspect'].unique()).to_csv('/Users/jiaxuan/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/MUP_year2/dissertation/datasets/nyc/qgis/final_aspect_results/aspect_keywords.csv')\n",
    "pd.DataFrame(df_fl_en_notnull['description'].unique()).to_csv('/Users/jiaxuan/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/MUP_year2/dissertation/datasets/nyc/qgis/final_aspect_results/description_keywords.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "join coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = pd.read_csv(\"/Users/jiaxuan/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/MUP_year2/dissertation/datasets/nyc/listings.csv\")\n",
    "listings.rename(columns={'id':'listing_id'}, inplace = True)\n",
    "listings_coord = listings[['listing_id', 'latitude','longitude']]\n",
    "df_aspect_coord = pd.merge(df_fl_en_notnull, listings_coord, on = ['listing_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "group by aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transport = df_aspect_coord[df_aspect_coord['aspect'].isin(transport)]\n",
    "df_amenities = df_aspect_coord[df_aspect_coord['aspect'].isin(amenities)]\n",
    "df_greenery = df_aspect_coord[df_aspect_coord['aspect'].isin(greenery)]\n",
    "df_safety = df_aspect_coord[df_aspect_coord['aspect'].isin(safety)]\n",
    "df_noise = df_aspect_coord[df_aspect_coord['aspect'].isin(noise)]\n",
    "\n",
    "df_transport_des = df_aspect_coord[df_aspect_coord['description'].isin(transport_des)] \n",
    "df_amenities_des = df_aspect_coord[df_aspect_coord['description'].isin(amenities_des)]\n",
    "df_greenery_des = df_aspect_coord[df_aspect_coord['description'].isin(greenery_des)]\n",
    "df_safety_des = df_aspect_coord[df_aspect_coord['description'].isin(safety_des)]\n",
    "df_noise_des = df_aspect_coord[df_aspect_coord['description'].isin(noise_des)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transport_final = pd.concat([df_transport,df_transport_des]).drop_duplicates()\n",
    "df_amenities_final = pd.concat([df_amenities,df_amenities_des]).drop_duplicates()\n",
    "df_greenery_final = pd.concat([df_greenery,df_greenery_des]).drop_duplicates()\n",
    "df_safety_final = pd.concat([df_safety,df_safety_des]).drop_duplicates()\n",
    "df_noise_final = pd.concat([df_noise,df_noise_des]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transport_final.to_csv(\"/Users/jiaxuan/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/MUP_year2/dissertation/datasets/nyc/qgis/transport.csv\", index = False)\n",
    "df_amenities_final.to_csv(\"/Users/jiaxuan/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/MUP_year2/dissertation/datasets/nyc/qgis/amenities.csv\", index = False)\n",
    "df_greenery_final.to_csv(\"/Users/jiaxuan/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/MUP_year2/dissertation/datasets/nyc/qgis/greenery.csv\", index = False)\n",
    "df_safety_final.to_csv(\"/Users/jiaxuan/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/MUP_year2/dissertation/datasets/nyc/qgis/safety.csv\", index = False)\n",
    "df_noise_final.to_csv(\"/Users/jiaxuan/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/MUP_year2/dissertation/datasets/nyc/qgis/noise.csv\", index = False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e09ce0fd82ae9bf1c51f28e30b29086463be244605f516ca9637e8a388375f1e"
  },
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit ('3.6.5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
